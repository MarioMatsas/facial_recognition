{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow==2.10 opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard dependencies\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np # For re-shaping arrays\n",
        "from matplotlib import pyplot as plt # Visualise images\n",
        "\n",
        "# Tensorflow dependencies\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Conv2D, Dense, MaxPooling2D, Input, Flatten, Lambda\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To avoid running out of memory, we restrict the GPU memory growth aka\n",
        "# how many resources the model is consuming at any given time\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    # Just in case the device we are using has more than one gpu, we are \n",
        "    # making sure to restrict the usage of ALL of them\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpus # Just to make sure that the GPU device is recognized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create files that will hold the anchor, positive and negative images:\n",
        "# Anchor: The image we imput\n",
        "# Positive: Images that match the anchor\n",
        "# Negative: Images that are different from the anchor\n",
        "anc_path = os.path.join(\"data\",\"anchor\")\n",
        "pos_path = os.path.join(\"data\",\"positive\")\n",
        "neg_path = os.path.join(\"data\",\"negative\")\n",
        "\n",
        "os.makedirs(anc_path)\n",
        "os.makedirs(pos_path)\n",
        "os.makedirs(neg_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncompress the tar file that contains the images you need to download from the following link:\n",
        "!tar -xf lfw.tgz # http://vis-www.cs.umass.edu/lfw/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We take the images that we downloaded and place in the negative folder\n",
        "# (these images will be used so that the machine can understand that the\n",
        "# person whose image we are providing isn't the same as any of the ones in\n",
        "# the negative folder)\n",
        "\n",
        "# Go through all the directories in the lfw folder\n",
        "for directory in os.listdir('lfw'):\n",
        "    # Find all the images in said directory\n",
        "    for file in os.listdir(os.path.join('lfw',directory)):\n",
        "        # Replace the path of that image with the path of the negative folder\n",
        "        # (aka place the image in the negative folder)\n",
        "        previous_path = os.path.join('lfw', directory, file)\n",
        "        new_path = os.path.join(neg_path, file)\n",
        "        os.replace(previous_path, new_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing this so that the images we save all have different names\n",
        "import uuid\n",
        "\n",
        "# Now we will get the images we will require for the anchor and positive files\n",
        "\n",
        "# Connect to the webcam\n",
        "capture = cv2.VideoCapture(0) # Keep in mind this number might vary slightly\n",
        "                              # so try out a few other numbers like 1, 2, 3, 4, 5 etc in case there is a problem\n",
        "while (capture.isOpened()):\n",
        "    return_value, frame = capture.read()\n",
        "\n",
        "    # Keeping in mind that the images in the negative folder have a resolution of 250x250\n",
        "    # we need out frames (aka the images we will capture) to be 250x250 as well\n",
        "    frame = frame[120:370, 200:450, :]\n",
        "\n",
        "    # Show the camera feed\n",
        "    cv2.imshow(\"Images\", frame)\n",
        "\n",
        "    # Add image to anchor if 'a' is pressed\n",
        "    if (cv2.waitKey(1) & 0XFF == ord('a')):\n",
        "        # Create the unique name and save the image\n",
        "        name = os.path.join(anc_path, '{}.jpg'.format(uuid.uuid1()))\n",
        "        cv2.imwrite(name, frame)\n",
        "    \n",
        "    # Add image to positive if 'p' is pressed\n",
        "    if (cv2.waitKey(1) & 0XFF == ord('p')):\n",
        "        # Create the unique name and save the image\n",
        "        name = os.path.join(pos_path, '{}.jpg'.format(uuid.uuid1()))\n",
        "        cv2.imwrite(name, frame)\n",
        "\n",
        "    # Break by pressing he 'q' key\n",
        "    if (cv2.waitKey(1) & 0XFF == ord('q')):\n",
        "        break\n",
        "\n",
        "# Release webcam\n",
        "capture.release()\n",
        "# Close the camera feed window\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 3000 image paths from each image set\n",
        "anchor = tf.data.Dataset.list_files(anc_path+'\\*.jpg').take(3000)\n",
        "positive = tf.data.Dataset.list_files(pos_path+'\\*.jpg').take(3000)\n",
        "negative = tf.data.Dataset.list_files(neg_path+'\\*.jpg').take(3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir_test = anchor.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'data\\\\anchor\\\\2d257bc5-547c-11ee-a668-38d57a328974.jpg'\n"
          ]
        }
      ],
      "source": [
        "print(dir_test.next())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale and resize the images\n",
        "def preprocess(file_path):\n",
        "    # Get byte code of image (the file path) and then decode it\n",
        "    byte_img = tf.io.read_file(file_path)\n",
        "    img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "    img = tf.image.resize(img, (105, 105)) # Resizing out image according to the \"Siamese Neural Networks\"\n",
        "                                           # research paper\n",
        "    img = img / 255.0 # Scale every pixel value to 0-1 => scale the image\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = preprocess('data\\\\anchor\\\\47c0c4c7-547c-11ee-b25f-38d57a328974.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img.numpy().max() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Depending on the inputs (anchor, positive) or (anchor, negative) we will be\n",
        "# getting a result ( a label ) as follows:\n",
        "# (anchor, positive) => 1\n",
        "# (anchor, negative) => 0\n",
        "\n",
        "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
        "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
        "data = positives.concatenate(negatives) # Combine the positives and negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "samples = data.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex = samples.next()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(b'data\\\\anchor\\\\4d3f92d9-547c-11ee-801d-38d57a328974.jpg',\n",
              " b'data\\\\positive\\\\6394933b-547c-11ee-bfa9-38d57a328974.jpg',\n",
              " 1.0)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create function to scale and resize both images we pass\n",
        "def twin_preprocess(anc, verification_image, label):\n",
        "    return (preprocess(anc), preprocess(verification_image), label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = twin_preprocess(*ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(res[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataloader pipeline\n",
        "data = data.map(twin_preprocess)\n",
        "data = data.cache()\n",
        "data = data.shuffle(buffer_size=10000) # Simply mix the positive and negative images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training partition\n",
        "train_data = data.take((round(len(data)*0.7))) # Get 70% of the samples\n",
        "train_data = train_data.batch(16) # Pass 16 images each time\n",
        "train_data = train_data.prefetch(8) # Preprocess the next image beforehand\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing Partition\n",
        "test_data = data.skip((round(len(data)*0.7)))\n",
        "test_data = test_data.take((round(len(data)*0.3)))\n",
        "test_data = test_data.batch(16)\n",
        "test_data = test_data.prefetch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding layer\n",
        "\n",
        "def make_embedding():\n",
        "    # Input\n",
        "    inp = Input(shape=(105, 105, 3), name=\"input_image\")\n",
        "\n",
        "    # First block\n",
        "    # Convolusion layer\n",
        "    c1 = Conv2D(64, (10, 10), activation=\"relu\")(inp)\n",
        "    # Max pooling layer\n",
        "    m1  = MaxPooling2D(64, (2, 2), padding=\"same\")(c1)\n",
        "\n",
        "    # Second block\n",
        "    c2 = Conv2D(128, (7, 7), activation=\"relu\")(m1)\n",
        "    m2 = MaxPooling2D(64, (2, 2), padding=\"same\")(c2)\n",
        "\n",
        "    # Third block\n",
        "    c3 = Conv2D(128, (4, 4), activation=\"relu\")(m2)\n",
        "    m3 = MaxPooling2D(64, (2, 2), padding=\"same\")(c3)\n",
        "\n",
        "    # Final embedding block\n",
        "    c4 = Conv2D(256, (4, 4), activation=\"relu\")(m3)\n",
        "    f1 = Flatten()(c4)\n",
        "    d1 = Dense(4096, activation=\"sigmoid\")(f1)\n",
        "\n",
        "    return Model(inputs=[inp], outputs=[d1], name=\"embedding\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding = make_embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"embedding\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_image (InputLayer)     [(None, 105, 105, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 96, 96, 64)        19264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 42, 42, 128)       401536    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 18, 18, 128)       262272    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 6, 256)         524544    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              37752832  \n",
            "=================================================================\n",
            "Total params: 38,960,448\n",
            "Trainable params: 38,960,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "embedding.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Siamese model\n",
        "def make_siamese_model():\n",
        "    # Anchor input image\n",
        "    input_image = Input(name=\"input_img\", shape=(105, 105, 3))\n",
        "    # Positive / Negative input image\n",
        "    validation_image = Input(name=\"validation_img\", shape=(105, 105, 3))\n",
        "\n",
        "    # Calculate L1 distance between the encoded vectors\n",
        "    distances = Lambda(lambda x: tf.abs(x[0] - x[1]), name='l1_distance')([embedding(input_image), embedding(validation_image)])\n",
        "\n",
        "    # Classification layer\n",
        "    classifier = Dense(1, activation=\"sigmoid\")(distances)\n",
        "\n",
        "    return Model(inputs=[input_image, validation_image], outputs=classifier, name=\"Siamese_Network\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "siamese_model = make_siamese_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Siamese_Network\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_img (InputLayer)          [(None, 105, 105, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "validation_img (InputLayer)     [(None, 105, 105, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Functional)          (None, 4096)         38960448    input_img[0][0]                  \n",
            "                                                                 validation_img[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "l1_distance (Lambda)            (None, 4096)         0           embedding[2][0]                  \n",
            "                                                                 embedding[3][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            4097        l1_distance[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 38,964,545\n",
            "Trainable params: 38,964,545\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "siamese_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Siamese_Network\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_img (InputLayer)          [(None, 105, 105, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "validation_img (InputLayer)     [(None, 105, 105, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Functional)          (None, 4096)         38960448    input_img[0][0]                  \n",
            "                                                                 validation_img[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "l1_distance (Lambda)            (None, 4096)         0           embedding[2][0]                  \n",
            "                                                                 embedding[3][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            4097        l1_distance[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 38,964,545\n",
            "Trainable params: 38,964,545\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "siamese_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup loss function\n",
        "binary_cross_loss = tf.losses.BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer\n",
        "opt = tf.keras.optimizers.Adam(1e-4) # 1e - 4 = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checkpoints\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "os.mkdir(checkpoint_dir)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_batch = train_data.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_1 =test_batch.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = batch_1[:2]\n",
        "y = batch_1[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the training step for each batch of data based on the following steps:\n",
        "# 1. Make a prediction\n",
        "# 2. Calculate loss\n",
        "# 3. Derive gradients\n",
        "# 4. Calculate new weights and apply\n",
        "\n",
        "@tf.function # Compiles the function into callable TensorFlow graph\n",
        "             # aka it helps train the model efficiently\n",
        "def train_step(batch):\n",
        "    with tf.GradientTape() as tape: # Helps us can capture the garients\n",
        "        # Get anchor and positive/negative image\n",
        "        x = batch[:2]  # Each batch cointains 16 anchors, 16 positives/negatives and 16 labels\n",
        "        # Get label\n",
        "        y_true = batch[2]\n",
        "\n",
        "        y_pred = siamese_model(x, training=True) # Make a prediction\n",
        "                                               # **Training = True is importanyt to activate all the layers\n",
        "        # Calculate loss\n",
        "        loss = binary_cross_loss(y_true, y_pred) # Calculate loss (true value, prediction)\n",
        "    \n",
        "    # Calculate gradients with respect to the loss\n",
        "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
        "\n",
        "    # Update weights and apply to siamese model\n",
        "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
        "\n",
        "    #return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train loop\n",
        "def train(data, EPOCHS):\n",
        "    # Loop through epochs\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        print(\"'n Epoch {}/{}\".format(epoch, EPOCHS)) # Optional and simply for the visuals\n",
        "        progbar = tf.keras.utils.Progbar(len(data)) # -//-\n",
        "\n",
        "        # Look through each batch\n",
        "        for idx, batch in enumerate(data):\n",
        "            # Run train step\n",
        "            train_step(batch)\n",
        "            progbar.update(idx + 1) \n",
        "\n",
        "        # Save checkpoints\n",
        "        if epoch % 10 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train(train_data, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.metrics import Recall\n",
        "# Note: Precision demonstrates the proporion of correct positive identifications\n",
        "#       Recall demostrates the proportion of the ACTUAL positive that were correctly identified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "# Get a batch of data\n",
        "test_input, test_value, y_true = test_data.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = siamese_model.predict([test_input, test_value])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[9.9870914e-01],\n",
              "       [1.3241939e-07],\n",
              "       [1.0000000e+00],\n",
              "       [5.5191106e-13],\n",
              "       [9.9999988e-01],\n",
              "       [9.9988294e-01],\n",
              "       [5.2315460e-07],\n",
              "       [9.9999607e-01],\n",
              "       [9.9962521e-01],\n",
              "       [9.9996722e-01],\n",
              "       [3.1029652e-12],\n",
              "       [6.7798722e-10],\n",
              "       [9.9998313e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.2673785e-13],\n",
              "       [1.1804138e-11]], dtype=float32)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[1 if prediction > 0.5 else 0 for prediction in y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Post processing the results (aka we create a threshhold so that any numbers above that \n",
        "# become => 1 otherwise => 0)\n",
        "[1 if prediction > 0.5 else 0 for prediction in y_pred]\n",
        "\n",
        "# Compare the above list to the labels to see if the outputs match the real results\n",
        "m = Recall()\n",
        "\n",
        "# Calculate the recall value\n",
        "m.update_state(y_true,y_pred)\n",
        "\n",
        "# Return the recall result (pretty much how accurate the model is)\n",
        "# 1 = perfect  |   0 = horibble\n",
        "m.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualise the results\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(test_input[1])\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(test_value[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Model\n",
        "siamese_model.save(\"siamesemodelx.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload Model\n",
        "l_model = tf.keras.models.load_model(\"siamesemodelx.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Siamese_Network\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_img (InputLayer)         [(None, 105, 105, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " validation_img (InputLayer)    [(None, 105, 105, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
            "                                                                  'validation_img[0][0]']         \n",
            "                                                                                                  \n",
            " l1_distance (Lambda)           (None, 4096)         0           ['embedding[0][0]',              \n",
            "                                                                  'embedding[1][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            4097        ['l1_distance[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 38,964,545\n",
            "Trainable params: 38,964,545\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "l_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "app_ver_path = os.path.join(\"application_data\", \"verification_images\")\n",
        "app_inp_path = os.path.join(\"application_data\", \"input_image\")\n",
        "\n",
        "os.makedirs(app_ver_path)\n",
        "os.makedirs(app_inp_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all files in the source directory\n",
        "all_files = os.listdir(pos_path)\n",
        "\n",
        "# Randomly select 50 unique files\n",
        "selected_files = random.sample(all_files, 50)\n",
        "\n",
        "# Iterate through the selected files and copy them to the destination directory\n",
        "for filename in selected_files:\n",
        "    source_file = os.path.join(pos_path, filename)\n",
        "    destination_file = os.path.join(app_ver_path, filename)\n",
        "    shutil.copy2(source_file, destination_file)\n",
        "    #print(f'Copied: {filename} to {destination_directory}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify(model, detection_threshold, verification_threshold):\n",
        "    # Build results array\n",
        "    results = []\n",
        "    for image in os.listdir(os.path.join(app_ver_path)):\n",
        "        input_img = preprocess(os.path.join(\"application_data\", \"input_image\", \"input_image.jpg\"))\n",
        "        validation_img = preprocess(os.path.join(\"application_data\", \"verification_images\", image))\n",
        "\n",
        "        # Make predictions\n",
        "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis = 1)))\n",
        "        results.append(result)\n",
        "    \n",
        "    # Detection Threshold: A metric above which a prediction is considered positive\n",
        "    detection = np.sum(np.array(results) > detection_threshold)\n",
        "    \n",
        "    # Verification Threshold: Proportion of positive predictions in regards to the total positive samples\n",
        "    verification = detection / len(os.listdir(os.path.join(\"application_data\", \"verification_images\")))\n",
        "    verified = verification > verification_threshold\n",
        "    \n",
        "    return results, verified\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(os.listdir(os.path.join(\"application_data\", \"verification_images\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenCV Real Time Verification\n",
        "cap = cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    frame = frame[120:370, 200:450, :]\n",
        "\n",
        "    cv2.imshow(\"Verification\", frame)\n",
        "\n",
        "    # Verification trigger\n",
        "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
        "        # Save input image to application_data/input_image folder\n",
        "        cv2.imwrite(os.path.join(\"application_data\", \"input_image\", \"input_image.jpg\"), frame)\n",
        "\n",
        "        # Run verification l_model\n",
        "        results, verified = verify(siamese_model, 0.5, 0.5)\n",
        "        print(verified)\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
